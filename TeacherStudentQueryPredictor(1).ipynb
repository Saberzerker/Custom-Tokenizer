{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6YvZLbpwiEUk"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
        "import torch\n",
        "from huggingface_hub import login\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EmbeddingCalculator:\n",
        "    \"\"\"\n",
        "    Calculates embeddings for text using a Hugging Face model.\n",
        "    \"\"\"\n",
        "    def __init__(self, model_name, hf_token):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=hf_token)\n",
        "        self.model = AutoModel.from_pretrained(\n",
        "            model_name,\n",
        "            use_auth_token=hf_token,\n",
        "            device_map=\"auto\",\n",
        "            torch_dtype=torch.float16\n",
        "        )\n",
        "\n",
        "    def calculate_embedding(self, text):\n",
        "        \"\"\"\n",
        "        Calculate the embedding for the given text.\n",
        "        \"\"\"\n",
        "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "        inputs = {key: value.to(self.model.device) for key, value in inputs.items()}\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs, output_hidden_states=True)\n",
        "            embedding = outputs.hidden_states[-1][:, 0, :].squeeze().cpu().numpy()\n",
        "        return embedding\n",
        "\n"
      ],
      "metadata": {
        "id": "Jx9lYINpiMZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class EnhancedDynamicMemory:\n",
        "    \"\"\"\n",
        "    Stores embeddings dynamically and allows similarity-based retrieval.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.memory = {}\n",
        "\n",
        "    def add_embedding(self, key, embedding, weight=1.0):\n",
        "        \"\"\"\n",
        "        Add or update an embedding in the dynamic memory.\n",
        "        Combines embeddings if the key already exists (weighted average).\n",
        "        \"\"\"\n",
        "        if key in self.memory:\n",
        "            # Combine existing and new embeddings\n",
        "            old_embedding = self.memory[key][\"embedding\"]\n",
        "            old_weight = self.memory[key][\"weight\"]\n",
        "            combined_embedding = (old_embedding * old_weight + embedding * weight) / (old_weight + weight)\n",
        "            self.memory[key] = {\"embedding\": combined_embedding, \"weight\": old_weight + weight}\n",
        "        else:\n",
        "            self.memory[key] = {\"embedding\": embedding, \"weight\": weight}\n",
        "\n",
        "    def find_closest_embedding(self, embedding, threshold=0.8):\n",
        "        \"\"\"\n",
        "        Find the closest embedding in memory using cosine similarity.\n",
        "        Returns the key and similarity score if above the threshold, otherwise None.\n",
        "        \"\"\"\n",
        "        max_similarity = 0\n",
        "        closest_key = None\n",
        "\n",
        "        for key, value in self.memory.items():\n",
        "            stored_embedding = value[\"embedding\"]\n",
        "            similarity = cosine_similarity([embedding], [stored_embedding])[0][0]\n",
        "            if similarity > max_similarity:\n",
        "                max_similarity = similarity\n",
        "                closest_key = key\n",
        "\n",
        "        return closest_key, max_similarity if max_similarity >= threshold else None\n",
        "\n",
        "    def visualize_memory(self):\n",
        "        \"\"\"\n",
        "        Display the keys and weights of stored embeddings.\n",
        "        \"\"\"\n",
        "        for key, value in self.memory.items():\n",
        "            print(f\"Key: {key}, Weight: {value['weight']}\")\n"
      ],
      "metadata": {
        "id": "s7U9XuuQiMPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NextQueryPredictor:\n",
        "    \"\"\"\n",
        "    Predicts the next queries using a Hugging Face causal language model.\n",
        "    \"\"\"\n",
        "    def __init__(self, model_name):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            device_map=\"auto\",\n",
        "            torch_dtype=torch.float16\n",
        "        )\n",
        "\n",
        "    def predict_next_queries(self, context, num_queries=5):\n",
        "        \"\"\"\n",
        "        Predict the next queries based on the given context.\n",
        "        \"\"\"\n",
        "        prompt = f\"\"\"\n",
        "You are tasked with predicting the next {num_queries} queries based on the following context:\n",
        "{context}\n",
        "\n",
        "Provide the next {num_queries} queries as a numbered list:\n",
        "\"\"\"\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "        inputs = {key: value.to(self.model.device) for key, value in inputs.items()}\n",
        "\n",
        "        outputs = self.model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            max_length=300,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            num_return_sequences=1\n",
        "        )\n",
        "\n",
        "        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        return self._extract_numbered_list(response)\n",
        "\n",
        "    def _extract_numbered_list(self, response):\n",
        "        \"\"\"\n",
        "        Extract a numbered list from the model's response.\n",
        "        \"\"\"\n",
        "        import re\n",
        "        numbered_lines = re.findall(r\"^\\d+\\..*\", response, re.MULTILINE)\n",
        "        return [line.split(\". \", 1)[1] for line in numbered_lines]"
      ],
      "metadata": {
        "id": "ag0vqh-UiMMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_hits_and_accuracy(user_queries, predicted_queries_list):\n",
        "    \"\"\"\n",
        "    Calculate hits, misses, and accuracy based on user queries and predicted queries.\n",
        "    \"\"\"\n",
        "    hits = 0\n",
        "    misses = 0\n",
        "\n",
        "    for i, user_query in enumerate(user_queries):\n",
        "        if i < len(predicted_queries_list):\n",
        "            predicted_queries = predicted_queries_list[i]\n",
        "            if user_query in predicted_queries:\n",
        "                hits += 1\n",
        "            else:\n",
        "                misses += 1\n",
        "\n",
        "    total_queries = len(user_queries)\n",
        "    accuracy = (hits / total_queries) * 100 if total_queries > 0 else 0\n",
        "\n",
        "    return hits, misses, accuracy"
      ],
      "metadata": {
        "id": "fPmXOKw3iMKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dynamic_teacher_student_logic(user_query, embedding_calculator, memory, context):\n",
        "    \"\"\"\n",
        "    Implements dynamic teacher-student interaction with hit-or-miss logic.\n",
        "    \"\"\"\n",
        "    # Calculate embedding for the new query\n",
        "    new_embedding = embedding_calculator.calculate_embedding(user_query)\n",
        "\n",
        "    # Check for a hit or miss in memory\n",
        "    closest_key, similarity = memory.find_closest_embedding(new_embedding)\n",
        "\n",
        "    if closest_key:\n",
        "        print(f\"Hit: Found similar query with similarity {similarity:.2f}\")\n",
        "        print(f\"Closest Query: {closest_key}\")\n",
        "        # Refine embedding for this query\n",
        "        memory.add_embedding(closest_key, new_embedding)\n",
        "    else:\n",
        "        print(\"Miss: No similar query found. Adding new query to memory.\")\n",
        "        memory.add_embedding(user_query, new_embedding)\n",
        "\n",
        "    # Update the context with the user's query and a placeholder answer\n",
        "    context += f\"Q: {user_query}\\nA: (Generated Answer)\\n\"\n",
        "    return context"
      ],
      "metadata": {
        "id": "GUH9l946iMIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Authenticate with Hugging Face\n",
        "    print(\"Authenticating with Hugging Face...\")\n",
        "    login()\n",
        "    hf_token = input(\"Enter your Hugging Face token: \").strip()\n",
        "\n",
        "    # Model names\n",
        "    embedding_model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "    query_predictor_model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "\n",
        "    # Initialize components\n",
        "    embedding_calculator = EmbeddingCalculator(embedding_model_name, hf_token)\n",
        "    memory = EnhancedDynamicMemory()\n",
        "    query_predictor = NextQueryPredictor(query_predictor_model_name)\n",
        "\n",
        "    # Initialize tracking variables\n",
        "    user_queries = []\n",
        "    predicted_queries_list = []\n",
        "    context = \"\"  # Context for predictions\n",
        "\n",
        "    print(\"\\n=== Interactive Q&A Session ===\")\n",
        "    print(\"Ask your questions to the system.\")\n",
        "    print(\"Type 'exit' to end the session.\\n\")\n",
        "\n",
        "    while True:\n",
        "        # Get user input (actual query)\n",
        "        user_query = input(\"You: \").strip()\n",
        "        if user_query.lower() == \"exit\":\n",
        "            break\n",
        "\n",
        "        # Append the user query to the tracking list\n",
        "        user_queries.append(user_query)\n",
        "\n",
        "        # Generate predicted queries in the background\n",
        "        predicted_queries = query_predictor.predict_next_queries(context)\n",
        "        predicted_queries_list.append(predicted_queries)\n",
        "\n",
        "        # Process the user query with teacher-student logic\n",
        "        context = dynamic_teacher_student_logic(user_query, embedding_calculator, memory, context)\n",
        "\n",
        "        # Simulate system's response\n",
        "        print(\"System: (Generated Answer)\")\n",
        "\n",
        "    # Calculate hits, misses, and accuracy\n",
        "    hits, misses, accuracy = calculate_hits_and_accuracy(user_queries, predicted_queries_list)\n",
        "\n",
        "    print(\"\\n=== Session Summary ===\")\n",
        "    print(f\"Total User Queries: {len(user_queries)}\")\n",
        "    print(f\"Hits: {hits}\")\n",
        "    print(f\"Misses: {misses}\")\n",
        "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "    # Visualize memory\n",
        "    print(\"\\nMemory Contents:\")\n",
        "    memory.visualize_memory()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "ihpCONCKiMGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iv38nXVQiL-8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}